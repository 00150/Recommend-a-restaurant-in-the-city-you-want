# # 1. ì…€ë ˆë‹ˆì›€ ì‘ë™í™•ì¸ ì™„ë£Œ
# import selenium
# from selenium import webdriver
# from selenium.webdriver.common.keys import Keys
# driver =webdriver.Chrome()

# 2. ì…€ë ˆë‹ˆì›€ ê²½ë¡œ ì‘ì„±í•˜ì—¬ ì‘ë™í™•ì¸.(ì™„ë£Œ)

# import selenium
# from selenium import webdriver
# from selenium.webdriver.common.keys import Keys
# driver = webdriver.Chrome(r'C:\Users\j.park\Section3\real_project3\chromedriver.exe')


# #ë„ì ë„ì 
# import pandas as pd
# df = pd.read_csv('SELECT_REGION.csv', encoding='cp949')

# #ğŸ”† ì…€ë ˆë‹ˆì›€ : ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
# from selenium import webdriver
# from selenium.webdriver.common.keys import Keys
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from selenium.common.exceptions import TimeoutException
# from selenium.common.exceptions import NoSuchElementException
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import Select
# import time


# # ì…€ë ˆë‹ˆì›€ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‹¤ìš´ë¡œë“œí•œ í¬ë¡¬ ë“œë¼ì´ë²„ì˜ ê²½ë¡œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. 
# # â€» í˜„ì¬ ì‘ì—…ì¤‘ì— ìˆëŠ” ì½”ë“œíŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì— í•¨ê»˜ ìœ„ì¹˜í•˜ëŠ” ê²ƒì´ ë² ìŠ¤íŠ¸ì¸ ê²ƒ ê°™ìŠ´ë‹¤..
# chromedriver = r'C:\Users\j.park\Section3\real_project3\chromedriver.exe'
# driver = webdriver.Chrome(chromedriver)




# # í¬ë¡¤ë§ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì–»ì–´ì˜¬ ê³³ì€ ë„¤ì´ë²„ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
# # ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ì°½ì— [~ë™ ~~ì‹ë‹¹]ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’í˜€ì¤ë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë¯¸ë¦¬ ì„¤ì •í•˜ì—¬ì¤ì‹œë‹¤.
# df['ë„¤ì´ë²„_í‚¤ì›Œë“œ'] = df['í–‰ì •ë™ëª…'] + "%20" +df['í–‰ì •ë™ëª…'] #â— "%20"ì€ ë„ì–´ì“°ê¸°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
# df['naver_map_url'] = ''

# # ë³¸ê²©ì ìœ¼ë¡œ ê°€ê²Œ ìƒì„¸í˜ì´ì§€ì˜ URLì„ ê°€ì ¸ì˜¤ë„ë¡ í•©ë‹ˆë‹¤.

# for i, keyword in enumerate(df['ë„¤ì´ë²„_í‚¤ì›Œë“œ'].tolist()):  #tolistë¥¼ ì´ìš©, ê°ì²´ ìƒì„±
#     print("ì´ë²ˆì— ì°¾ì„ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ :", i, f'/{df.shape[0]-1}í–‰', keyword) #csv íŒŒì¼ì˜ ì²«ë²ˆì§¸ í–‰ì— ì»¬ëŸ¼ì´ ë‹´ê²¨ìˆìœ¼ë¯€ë¡œ ì œì™¸.
    
#     try:
#         naver_map_search_url = f'https://m.map.naver.com/search2/search.naver?query={keyword}&sm=hty&style=v5'  # í˜„ì¬ ì£¼ì†ŒëŠ” ëª¨ë°”ì¼ì…ë‹ˆë‹¤.
#         driver.get(naver_map_search_url)    
#         time.sleep(3.5)    
#         df.iloc[i,-1] = driver.find_element_by_css_selector(
#             '#ct > div.search_listview._content._ctList > ul > li:nth-child(1) > div.item_info > a.a_item.a_item_distance._linkSiteview').get_attribute('data-cid')
#         # ë„¤ì´ë²„ ì§€ë„ ì‹œìŠ¤í…œì€ data-cidì— url íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•´ë‘ê³  ìˆì—ˆìŠµë‹ˆë‹¤.
#         # data-cid ë²ˆí˜¸ë¥¼ ë½‘ì•„ë‘ì—ˆë‹¤ê°€ ê¸°ë³¸ url í…œí”Œë¦¿ì— ë„£ì–´ ìµœì¢…ì ì¸ urlì„ ì™„ì„±í•˜ë©´ ëâ—
    
#     except Exception as e1:
#         if "li:nth-child(1)" in str(e1):
#             try:
#                 df.iloc[i,-1] =driver.find_element_by_css_selector("#ct > div.search_listview._content._ctList > ul > li:nth-child(1) > div.item_info > a.a_item.a_item_distance._linkSiteview")
#                 time.sleep(1)
            
#             except Exception as e2:
#                 print(e2)
#                 df.iloc[i, -1] = np.nan
#                 time.sleep(1)
        
#         else:
#             pass                    

# driver.quit()


# #ì´ë•Œ ìˆ˜ì§‘í•œ ê²ƒì€ ì™„ì „í•œ urlì´ ì•„ë‹ˆë¼ urlì— ë“¤ì–´ê°ˆ ID (data-cid ë¼ëŠ” ì½”ë“œëª…ìœ¼ë¡œ ì €ì¥ëœ ê²ƒ)ì´ë¯€ë¡œ, ì˜¨ì „í•œ URLë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.
# df['naver_map_url'] = "https://m.place.naver.com/restaurant/" + df['naver_map_url']

# #urlì´ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ë°ì´í„°ëŠ” ì œê±°í•´ì¤€ë‹¤.
# df = df.loc[~df['naver_map_url'].isnull()]


# íŒ¨í‚¤ì§€ ì¶”ê°€ : tqdm  (íŒ¨í‚¤ì§€ ì‚¬ìš©ì „ ì„¤ì¹˜ í•„ìˆ˜ ì…ë‹ˆë‹¤.)


from tqdm.notebook import tqdm

#ğŸ”† ì…€ë ˆë‹ˆì›€ : ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
import time
import pandas as pd
import numpy as np
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

naver_store_name_type = []  
blog_review_list = []
blog_review_count_list = []
naver_star_review_list = []
naver_visitor_review_list = []


s = Service(r'C:\Users\j.park\Section3\real_project3\chromedriver.exe')

# ë©”ì¸ ë“œë¼ì´ë²„ : ë³„ì ë“±ì„ í¬ë¡¤ë§
driver = webdriver.Chrome(service=s)

# ì„œë¸Œ ë“œë¼ì´ë²„ : ë¸”ë¡œê·¸ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ë¦¬ë·° íƒ­ì— ë“¤ì–´ê°€ì„œ í¬ë¡¤ë§
sub_driver = webdriver.Chrome(service=s)


# Part.2 ì—ì„œ í¬ë¡¤ë§í•˜ì—¬ ëª¨ì€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
crawling_df = pd.read_csv('first_crawling.csv', encoding= 'cp949')


def second_crawling(df_column):
  for i, url in enumerate(tqdm(df_column)): #tqdm : ì‘ì—… ê³¼ì •ì˜ ì§„í–‰ìƒí™©ì„ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    driver.get(url)
    sub_driver.get(url+"/review/urgc")
    time.sleep(2)
  
    try:
      
      # ê°„ë‹¨í•œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°.
      
      # ê°€ê²Œì˜ ìœ í˜• ë¶„ë¥˜
      naver_store_type = driver.find_element_by_css_selector("#_title > span._3ocDE").text
      
      # ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜
      blog_review_count = driver.find_element_by_css_selector('#app-root > div > div > div > div.place_section.GCwOh > div._3uUKd > div._37n49 > span:nth-child(3) > a > em').text
      
      # ë°©ë¬¸ì ë¦¬ë·° ìˆ˜
      visitor_review_count = driver.find_element_by_css_selector("#app-root > div > div > div > div.place_section.GCwOh > div._3uUKd > div._37n49 > span:nth-child(2) > a > em").text
      
      #ê°€ê²Œ ë³„ì  ì ìˆ˜
      review_stars = driver.find_element_by_css_selector("#app-root > div > div > div > div.place_section.GCwOh > div._3uUKd > div._37n49 > span._1Y6hi._1A8_M > em").text

      # ë¦¬ë·° í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°.
      review_text_list = [] # ì„ì‹œ ì„ ì–¸
      
      # ë„¤ì´ë²„ ì§€ë„ ë¸”ë¡œê·¸ ë¦¬ë·° íƒ­
      # ë™ì  ì›¹ì‚¬ì´íŠ¸ì˜ ìˆœì„œê°€ ì£¼ë¬¸í•˜ê¸°, ë©”ë‰´ë³´ê¸° ë“±ì˜ ì¡´ì¬ ì—¬ë¶€ë¡œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— css selectorê°€ ì•„ë‹ˆë¼ element ì°¾ê¸°ë¡œ ì§„í–‰í•œë‹¤.
      # ì¦‰ ë¦¬ë·°íƒ­ì˜ ìš”ì†Œë¥¼ ì‘ì„±í•œë‹¤.
      review_text_crawl_list = sub_driver.find_elements_by_class_name("_3Q5_9")
      
      
      # find_elements_by_class_name ë©”ì†Œë“œë¥¼ í†µí•´  ê°€ì ¸ì˜¨ ë‚´ìš©ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ëœë‹¤.
      # ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì„ í’€ì–´ì„œ ì„ì‹œ ë°ì´í„°ì— ëª¨ì•„ ë‘ì–´ì•¼ í•œë‹¤.(forë¬¸ ì‚¬ìš©)
      for review_crawing_data in review_text_crawl_list:
        
        # ì„ì‹œë¡œ ì„ ì–¸ëœ review_text_listì— ë‹´ê¸°.
        review_text_list.append(review_crawing_data.find_element_by_tag_name("WoYOw").text)
      
      #ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ëœ í…ìŠ¤íŠ¸(í•œ ì‹ë‹¹ì— ëŒ€í•œ ì—¬ëŸ¬ ë¦¬ë·°ë“¤)ë¥¼ ë©ì–´ë¦¬ë¡œ ëª¨ì•„ì¤ë‹ˆë‹¤.
      review_text = ",".join(review_text_list)
      
      
      blog_review_list.append(review_text)
      naver_store_name_type.append(naver_store_type)  
      blog_review_count_list.append(blog_review_count)
      naver_star_review_list.append(review_stars)
      naver_visitor_review_list.append(visitor_review_count)
      
    # ë¦¬ë·°ê°€ ì—†ëŠ” ì—…ì²´ëŠ” í¬ë¡¤ë§ì— ì˜¤ë¥˜ê°€ ëœ¨ë¯€ë¡œ ì •ë¦¬í•  ê²ƒ. 
    except Exception as e1:
      print(f'{i}í–‰ì— ë¬¸ì œê°€ ë°œìƒ')
      
      #ë¦¬ë·°ê°€ ì—†ìœ¼ë¯€ë¡œ  nullì„ ì„ì‹œë¡œ ë„£ì–´ì¤€ë‹¤.
      blog_review_list.append('null')
      naver_store_name_type.append('null')
      blog_review_count_list.append('null')
      naver_star_review_list.append('null')
      naver_visitor_review_list.append('null')
      
  driver.quit()
  sub_driver.quit()
  
  df['naver_store_type'] = naver_store_name_type
  df['naver_star_point'] = naver_star_review_list
  df['naver_blog_review_count'] = blog_review_count_list
  df['naver_blog_review_text'] = blog_review_list
  df['naver_visitor_review_count'] = naver_visitor_review_list
  
  #ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ csv í˜•íƒœë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
  df.to_csv('c:/Users/j.park/Section3/real_project3/second_crawling.csv', index = False, encoding= 'cp949')
  
  print('mission completeâ—') 
  return None


#test_second_crawling
#ì‹¤í–‰í•´ë³´ê³ , ì˜¤ë¥˜ê°€ ë‚œë‹¤ë©´ ê³ ì³ë´…ì‹œë‹¤.
second_crawling(crawling_df['naver_map_url'])