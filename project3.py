#0. ì‚¬ìš©í•  íŒ¨í‚¤ì§€ ê°€ì ¸ì˜¤ê¸°
from multiprocessing import connection
import pandas as pd
from soupsieve import select
import numpy as np
import os
import sqlite3
import psycopg2
import csv
import logging

#Part 1. ë°ì´í„° ì „ì²˜ë¦¬ 

#1. ì‚¬ìš©í•  ë°ì´í„°ì˜ ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°.
data = os.path.join(os.getcwd(),'ê²½ê¸°ìƒê¶Œì •ë³´.csv')
df = pd.read_csv(data)


#2. ìŒì‹ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ì¶œë ¥ ë° ì‚¬ìš©í•  ì»¬ëŸ¼ë§Œ ì§€ì •í•˜ê¸°.
df = df.loc[df['ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…'] == 'ìŒì‹']

columns =['ìƒí˜¸ëª…', 'ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…', 'ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…', 'í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…', 'í–‰ì •ë™ëª…', 'ìœ„ë„', 'ê²½ë„']
df = df[columns]


#3. ì…ë ¥í•˜ëŠ” ë™ì„ ê¸°ì¤€ìœ¼ë¡œ, ì›í•˜ëŠ” ë°ì´í„°ë§Œ ì¶œë ¥ ë° ë”°ë¡œ ì €ì¥í•˜ê¸°. 
#3-1. ì°¾ê³ ì í•˜ëŠ” ë™ì˜ ì´ë¦„ì„ ì…ë ¥ ë°›ìŠµë‹ˆë‹¤.

dong = input('ì°¾ê³ ìí•˜ëŠ” ë™ë„¤ì˜ ë™ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”ğŸ˜† : ').replace(" ", '').split(',') # ì€í–‰2ë™, ì¥í•­2ë™, ì™€ë™
select_dong = dong

# # ---> í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ì—¬ ì´ìš©í•˜ê² ìŠµë‹ˆë‹¤.
def mapping_address():
      #3-1. ì°¾ê³ ì í•˜ëŠ” ë™ì˜ ì´ë¦„ì„ ì…ë ¥ ë°›ìŠµë‹ˆë‹¤.
  #dong = input('ì°¾ê³ ìí•˜ëŠ” ë™ë„¤ì˜ ë™ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”ğŸ˜† : ').replace(" ", '').split(',') # ì€í–‰2ë™, ì¥í•­2ë™, ì™€ë™
  dong_name = select_dong

  #3-2. ê¸°ë³¸ ë°ì´í„° ìƒì„±
  default_data = pd.DataFrame(columns = {'ìƒí˜¸ëª…', 'ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…', 'ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…', 'í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…', 'í–‰ì •ë™ëª…', 'ìœ„ë„', 'ê²½ë„'})  

  #3-3. forë¬¸ì„ í†µí•´ ê¸°ì…í•œ ë™ì´ë¦„ìœ¼ë¡œë§Œ ë°ì´í„°ë¥¼ ë”°ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
  for number in range(len(dong_name)):
    add_df = df.loc[(df['í–‰ì •ë™ëª…'] == dong_name[number])]
    default_data = pd.concat([add_df, default_data])
  
  #3-4.ì»¬ëŸ¼ëª… ë‹¨ìˆœí™”
  # ì¶”ë ¤ì§„ ë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ë©´ ì»¬ëŸ¼ëª…ì˜ ì–´íœ˜ê°€ ì¡°ê¸ˆ ì–´ë µìŠµë‹ˆë‹¤. ë³´ë‹¤ ì´ìš©ì— í¸ë¦¬í•˜ë„ë¡ í¸ë¦¬í•œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.
  default_data.columns = ['ìƒí˜¸ëª…',
                          'ì—…ì¢…ì¤‘ë¶„ë¥˜ëª…',
                          'ì—…ì¢…ì†Œë¶„ë¥˜ëª…',
                          'í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…',
                          'í–‰ì •ë™ëª…',
                          'ìœ„ë„',
                          'ê²½ë„']

  
  #3-5. ì›í•˜ëŠ” ì§€ì—­ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°í”„ë ˆì„ì„ ë”°ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
  default_data.to_csv('c:/Users/j.park/Section3/real_project3/SELECT_REGION.csv', index = False, encoding= 'cp949')
  


#4. ì´í›„ ìƒì„±ëœ ë°ì´í„°ë¥¼ í´ë¼ìš°ë“œ ë°ì´í„° ì„œë¹„ìŠ¤ì— ì €ì¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
#   Postgre ë°ì´í„°ë² ì´ìŠ¤ ì„œë²„ì™€ ì—°ê²° -> í´ë¼ìš°ë“œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒì„±í•œ elephantDBë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.
#   ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•  ë•Œ, í•„ìš”í•œ ì •ë³´ë“¤ì„ ì‚¬ì „ì— ë³€ìˆ˜ì— ë‹´ì•„ ë†“ìŠµë‹ˆë‹¤.

host = 'castor.db.elephantsql.com'
user = 'iejeegfa'
password = 'qjLvYChc-r75m8BZoQFRgNzRWlhNfV4U'
database = 'iejeegfa'

#4-1. ì—°ê²° & í…Œì´ë¸” ìƒì„± & í…Œì´ë¸” ì‚½ì…ì„ ì§„í–‰í•  í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

def connect_sql():
  
  # ì—°ê²° ë° ì—ëŸ¬ ì œì–´ 
  try:
    connection = psycopg2.connect(
      host = host,
      user = user,
      database = database,
      password = password)
    cur = connection.cursor()
  
  except:
    logging.error("could not connect to rds")
  
  
  # í…Œì´ë¸” ìƒì„±.
  cur.execute("DROP TABLE IF EXISTS SELECT_REGION;")
  cur.execute("""CREATE TABLE SELECT_REGION(
    Id INTEGER,
    ìƒí˜¸ëª… VARCHAR(128),
    ì—…ì¢…ì¤‘ë¶„ë¥˜ëª… VARCHAR(128),
    ì—…ì¢…ì†Œë¶„ë¥˜ëª… VARCHAR(128),
    í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª… VARCHAR(128),
    í–‰ì •ë™ëª… VARCHAR(128),
    ìœ„ë„ FLOAT8,
    ê²½ë„ FLOAT8
  );""")
  
  
  # ë°ì´í„° ì‚½ì….  
  with open('SELECT_REGION.csv', 'r') as f:
    try:
      reader = csv.reader(f)
      next(reader)
      for id, row in enumerate(reader, start=1):
        cur.execute("""INSERT INTO SELECT_REGION(Id, ìƒí˜¸ëª…, ì—…ì¢…ì¤‘ë¶„ë¥˜ëª…,
                              ì—…ì¢…ì†Œë¶„ë¥˜ëª…, í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…, í–‰ì •ë™ëª…, ìœ„ë„, ê²½ë„) VALUES (%s, %s, %s, %s, %s, %s, %s, %s);""",
                              (id, row[0],row[1], row[2], row[3], row[4], row[5], row[6]))
      print('mission completeâ—')      
    
    except:
      logging.error("can't insert data")
      
  connection.commit()
  connection.close() 
  

#ğŸ”†  ì§€ê¸ˆê¹Œì§€ ë§Œë“  í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
#(1). ì›í•˜ëŠ” ì§€ì—­ë§Œ ë§µí•‘í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ(ìƒì„±)
#(2). ì´í›„ í´ë¼ìš°ë“œë°ì´í„° ì„œë¹„ìŠ¤ ì—°ê²°(Postgress) 
#     ìƒì„±ëœ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ ì¶œë ¥í•˜ì—¬ ë´…ì‹œë‹¤.

#(1). ë°ì´í„° ì¶”ì¶œ 
mapping_address()

#(2). DB ì—°ê²° ë° ë°ì´í„° ì‚½ì…
connect_sql()

#------------------------------------------ğŸ”°ğŸ”°ğŸ”°ğŸ”°ğŸ”° PART. 2 ğŸ”°ğŸ”°ğŸ”°ğŸ”°ğŸ”°------------------------------------------------
# ìš”ì‹ì—… ì „ì²´ì˜ ì¹´í…Œê³ ë¦¬ ë°ì´í„° í™œìš©í•˜ê¸°. ---> 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„' ì´ìš©í•˜ê¸°.
# ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì§€ì—­ë§Œì„ ë§µí•‘í•˜ì—¬ ë§Œë“  ë°ì´í„°ì˜ ì»¬ëŸ¼ë“¤ì¤‘, 'ì¤‘ë¶„ë¥˜ëª…'ê³¼ 'ì†Œë¶„ë¥˜ëª…'ì€ í™œìš©ê°€ì¹˜ê°€ í° ë°ì´í„°ì…ë‹ˆë‹¤.
# ì´ ë°ì´í„°ë“¤ì„ Cartegory ë°ì´í„°ë¼ê³  ì§€ì¹­í•˜ê² ìŠµë‹ˆë‹¤.

# ì´ ë°ì´í„°ë“¤ì„ í•˜ë‚˜ì˜ ì»¬ëŸ¼ìœ¼ë¡œ ëª°ì•„ë‘ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
# https://innate-clove-c5b.notion.site/Cosine-Similarity-664f643b74a54a338065b7a3869576ab(ë‚´ ë…¸ì…˜ -> ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì •ë¦¬)
#â— ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìë©´, í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ì‰½ê²Œ 'ë¹„ìŠ·í•¨ì˜ ì •ë„'ë¥¼ íŒŒì•…í•˜ì—¬ ì¤ë‹ˆë‹¤.


#1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°.
select_df = pd.read_csv('SELECT_REGION.csv', encoding= 'cp949')


#â— ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë“¤ì˜ ëª¨ìŠµì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
#->ì–‘ì‹            ì •í†µì–‘ì‹/ê²½ì–‘ì‹  
#->ì»¤í”¼ì /ì¹´í˜     ì»¤í”¼ì „ë¬¸ì /ì¹´í˜/ë‹¤ë°©
#->í•œì‹           í•œì‹/ë°±ë°˜/í•œì •ì‹


#â—ì´ëŸ° í‚¤ì›Œë“œë¥¼ ë‹´ì€ ë°ì´í„°ë“¤ì„ í•˜ë‚˜ì˜ ì»¬ëŸ¼ì— ëª°ì•„ë‘ë©´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#->ì–‘ì‹            ì •í†µì–‘ì‹/ê²½ì–‘ì‹          ------------> ì–‘ì‹ ì „í†µì–‘ì‹ ê²½ì–‘ì‹
#->ì»¤í”¼ì /ì¹´í˜     ì»¤í”¼ì „ë¬¸ì /ì¹´í˜/ë‹¤ë°©      ------------> ì»¤í”¼ì  ì¹´í˜ ì»¤í”¼ì „ë¬¸ì  ì¹´í˜ ë‹¤ë°©
#->í•œì‹           í•œì‹/ë°±ë°˜/í•œì •ì‹           ------------> í•œì‹ í•œì‹ ë°±ë°˜ í•œì •ì‹


#2. ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë“¤ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì£¼ëŠ” ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
select_df['í•©ì¹œë°ì´í„°'] = select_df['ì—…ì¢…ì¤‘ë¶„ë¥˜ëª…'] + select_df['ì—…ì¢…ì†Œë¶„ë¥˜ëª…'] +select_df['í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…'] 

#3. í•˜ë‚˜ë¡œ ë­‰ì³ì§„ ì¹´í…Œê³ ë¦¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ 'í”¼ì²˜ ë²¡í„°í™”'í•˜ê³  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.(ì‚¬ì´í‚·ëŸ°)
import sklearn
from sklearn.feature_extraction.text import CountVectorizer  # í”¼ì²´ ë²¡í„°í™”
from sklearn.metrics.pairwise import cosine_similarity  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„

#ğŸ”†CountVectorizer 
# 1. ë¬¸ì„œë¥¼ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤.
# 2. ê° ë¬¸ì„œì—ì„œ í† í°ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ì„¼ë‹¤.
# 3. ê° ë¬¸ì„œë¥¼ BOW ì¸ì½”ë”© ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.

count_vect_category = CountVectorizer(min_df=0, ngram_range=(1,2)) 
# min_df : ë‹¨ì–´ì¥ì— í¬í•¨ë˜ê¸° ìœ„í•œ ìµœì†Œ ë¹ˆë„ (â€» ê¸°ë³¸ê°’ : 1)
# ngram_range:  n-ê·¸ë¨ ë²”ìœ„ë¡œ ë‹¨ì–´ë¥¼ ëª‡ ê°œë¡œ í† í°í™” í• ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.


place_category = count_vect_category.fit_transform(select_df['í•©ì¹œë°ì´í„°'])
applied_cosine = cosine_similarity(place_category, place_category)
place_simi_cate_sorted_ind = applied_cosine.argsort()[:, ::-1]
#ì´ë ‡ê²Œ í•˜ë©´, ê°ê°ì˜ ë°ì´í„° vs ë°ì´í„°ë“¤ì´ ì„œë¡œ ì¹´í…Œê³ ë¦¬ í…ìŠ¤íŠ¸ê°€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ ë”°ì ¸ì¤ë‹ˆë‹¤.
#500ê°œì˜ ë°ì´í„°ê°€ ìˆë‹¤ë©´ 1ë²ˆì€ ìê¸° ìì‹ ê³¼ í•œ ë²ˆ ë¹„êµí•˜ê³ , ë‚˜ë¨¸ì§€ 499ê°œì™€ ë¹„êµë¥¼ í•˜ê²Œ ë©ë‹ˆë‹¤.
#ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ ìµœì¢… ì±„ì  ë‹¨ê³„ì—ì„œ í™œìš©í•©ë‹ˆë‹¤.

#4. í¬í„¸ì˜ ë¸”ë¡œê·¸ ë¦¬ë·°ë“±ì„ ì´ìš©í•˜ì—¬ ë³„ì  ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë°ì´í„°ë¡œ í™œìš©í•œë‹¤.
#   ê³µê³µë°ì´í„°ë¡œ í™•ë³´í•œ ìƒí˜¸ëª…, í–‰ì •ë™ ëª…ì„ ê²€ìƒ‰ì–´ë¡œ ë³€í™˜í•˜ì—¬ í¬í„¸ì— ê²€ìƒ‰í•˜ê¸°.
#   ì…€ë ˆëŠ„ í¬ë¡¤ëŸ¬ë¥¼ í†µí•´ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë‚˜ì˜¨ ë¸”ë¡œê·¸ ë¦¬ë·°, ë¸”ë¡œê·¸ ë³„ì  ë°ì´í„° í™•ë³´í•˜ê¸°.

#ğŸ”† ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
import time

# ì…€ë ˆë‹ˆì›€ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‹¤ìš´ë¡œë“œí•œ í¬ë¡¬ ë“œë¼ì´ë²„ì˜ ê²½ë¡œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. 
# â€» í˜„ì¬ ì‘ì—…ì¤‘ì— ìˆëŠ” ì½”ë“œíŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì— í•¨ê»˜ ìœ„ì¹˜í•˜ëŠ” ê²ƒì´ ë² ìŠ¤íŠ¸ì¸ ê²ƒ ê°™ìŠ´ë‹¤..
chromedriver = r'C:\Users\j.park\Section3\real_project3\chromedriver.exe'
driver = webdriver.Chrome(chromedriver)


# í¬ë¡¤ë§ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì–»ì–´ì˜¬ ê³³ì€ ë„¤ì´ë²„ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
# ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ì°½ì— [~ë™ ~~ì‹ë‹¹]ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’í˜€ì¤ë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë¯¸ë¦¬ ì„¤ì •í•˜ì—¬ì¤ì‹œë‹¤.
select_df['ë„¤ì´ë²„í‚¤ì›Œë“œ'] = select_df['í–‰ì •ë™ëª…'] + "%20" +select_df['ìƒí˜¸ëª…'] #â— "%20"ì€ ë„ì–´ì“°ê¸°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
select_df['naver_map_url'] = ''

# ë³¸ê²©ì ìœ¼ë¡œ ê°€ê²Œ ìƒì„¸í˜ì´ì§€ì˜ URLì„ ê°€ì ¸ì˜¤ë„ë¡ í•©ë‹ˆë‹¤.

for i, keyword in enumerate(select_df['ë„¤ì´ë²„í‚¤ì›Œë“œ'].tolist()):  #tolistë¥¼ ì´ìš©, ê°ì²´ ìƒì„±
  print("ì´ë²ˆì— ì°¾ì„ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ :", i, f'/{select_df.shape[0]-1}í–‰', keyword) #csv íŒŒì¼ì˜ ì²«ë²ˆì§¸ í–‰ì— ì»¬ëŸ¼ì´ ë‹´ê²¨ìˆìœ¼ë¯€ë¡œ ì œì™¸.
    
  try:
      naver_map_search_url = f'https://m.map.naver.com/search2/search.naver?query={keyword}&sm=hty&style=v5'  # í˜„ì¬ ì£¼ì†ŒëŠ” ëª¨ë°”ì¼ì…ë‹ˆë‹¤.
      driver.get(naver_map_search_url)    
      time.sleep(3.5)    
      select_df.iloc[i,-1] = driver.find_element_by_css_selector(
          '#ct > div.search_listview._content._ctList > ul > li:nth-child(1) > div.item_info > a.a_item.a_item_distance._linkSiteview').get_attribute('data-cid')
      # ë„¤ì´ë²„ ì§€ë„ ì‹œìŠ¤í…œì€ data-cidì— url íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•´ë‘ê³  ìˆì—ˆìŠµë‹ˆë‹¤.
      # data-cid ë²ˆí˜¸ë¥¼ ë½‘ì•„ë‘ì—ˆë‹¤ê°€ ê¸°ë³¸ url í…œí”Œë¦¿ì— ë„£ì–´ ìµœì¢…ì ì¸ urlì„ ì™„ì„±í•˜ë©´ ëâ—
  
  except Exception as e1:
      if "li:nth-child(1)" in str(e1):
          try:
              select_df.iloc[i,-1] =driver.find_element_by_css_selector("#ct > div.search_listview._content._ctList > ul > li:nth-child(1) > div.item_info > a.a_item.a_item_distance._linkSiteview")
              time.sleep(1)
          
          except Exception as e2:
              print(e2)
              select_df.iloc[i, -1] = np.nan
              time.sleep(1)
      
      else:
          pass                    

driver.quit()


#ì´ë•Œ ìˆ˜ì§‘í•œ ê²ƒì€ ì™„ì „í•œ urlì´ ì•„ë‹ˆë¼ urlì— ë“¤ì–´ê°ˆ ID (data-cid ë¼ëŠ” ì½”ë“œëª…ìœ¼ë¡œ ì €ì¥ëœ ê²ƒ)ì´ë¯€ë¡œ, ì˜¨ì „í•œ URLë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.
select_df['naver_map_url'] = "https://m.place.naver.com/restaurant/" + select_df['naver_map_url']

#urlì´ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ë°ì´í„°ëŠ” ì œê±°í•´ì¤€ë‹¤.
select_df = select_df.loc[~select_df['naver_map_url'].isnull()]


#ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ csv í˜•íƒœë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
select_df.to_csv('c:/Users/j.park/Section3/real_project3/first_crawling.csv', index = False, encoding= 'cp949')



# #------------------------------------------â— ë¬¸ì œì  ë°œê²¬ â—------------------------------------------------
# ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ csví˜•íƒœë¡œ ë³µì›í•˜ì—¬ ì‚´í´ë³´ë˜ ë„ì¤‘ ë¬¸ì œì ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
# ê°€ê²Œì˜ ì£¼ì†Œì™€ ì¡°ì‚¬ëœ í–‰ì •ë™ëª…ì´ ë‹¤ë¥¸ ê²½ìš°ì…ë‹ˆë‹¤. (Ex: ì‹ ì •ìƒíšŒëŠ” csv íŒŒì¼ì—ì„œëŠ” ê´´ì•ˆë™ì— ìœ„ì¹˜í•˜ì§€ë§Œ, ì‹¤ì œë¡œ ì°¾ì•„ë³´ë‹ˆ ì†Œì‚¬ë³¸ë™ì´ì—ˆìŠµë‹ˆë‹¤.)




# #------------------------------------------ğŸ”°ğŸ”°ğŸ”°ğŸ”°ğŸ”° PART. 3 ğŸ”°ğŸ”°ğŸ”°ğŸ”°ğŸ”°------------------------------------------------
# # PART.2ë¥¼ í†µí•´ ìˆ˜ì§‘í•œ ìƒì„¸í˜ì´ì§€ urlì„ ë‹¤ì‹œ 2ê°œì˜ í¬ë¡¤ëŸ¬ê°€ ëŒë©´ì„œ ë¸”ë¡œê·¸ ë¦¬ë·°ì™€ ë³„ì  ë°ì´í„°ë“±ì„ ìˆ˜ì§‘í•´ì¤ì‹œë‹¤.
# # ê° ë°ì´í„°ë“¤ì„ ë¯¸ë¦¬ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì€ ë‹¤ìŒ, ë§ˆì§€ë§‰ì— ë°ì´í„° í”„ë ˆì„ì— í•©ì³ì¤ì‹œë‹¤.


# íŒ¨í‚¤ì§€ ì¶”ê°€ : tqdm  (íŒ¨í‚¤ì§€ ì‚¬ìš©ì „ ì„¤ì¹˜ í•„ìˆ˜ ì…ë‹ˆë‹¤.)
from tqdm.notebook import tqdm



naver_store_name_type = []  
blog_review_list = []
blog_review_count_list = []
naver_star_review_list = []
naver_visitor_review_list = []

# ë©”ì¸ ë“œë¼ì´ë²„ : ë³„ì ë“±ì„ í¬ë¡¤ë§
driver = webdriver.Chrome(chromedriver)

# ì„œë¸Œ ë“œë¼ì´ë²„ : ë¸”ë¡œê·¸ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ë¦¬ë·° íƒ­ì— ë“¤ì–´ê°€ì„œ í¬ë¡¤ë§
sub_driver = webdriver.Chrome(chromedriver)


# Part.2 ì—ì„œ í¬ë¡¤ë§í•˜ì—¬ ëª¨ì€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
crawling_df = pd.read_csv('first_crawling.csv', encoding= 'cp949')


def second_crawling(df):
  for i, url in enumerate(tqdm(df)): #tqdm : ì‘ì—… ê³¼ì •ì˜ ì§„í–‰ìƒí™©ì„ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    driver.get(url)
    sub_driver.get(url+"/review/urgc")
    time.sleep(2)
  
    try:
      
      # ê°„ë‹¨í•œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°.
      
      # ê°€ê²Œì˜ ìœ í˜• ë¶„ë¥˜
      naver_store_type = driver.find_element_by_css_selector("#_title > span._3ocDE").text
      
      # ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜
      blog_review_count = driver.find_element_by_css_selector('#app-root > div > div > div > div.place_section.GCwOh > div._3uUKd > div._37n49 > span:nth-child(3) > a > em').text
      
      # ë°©ë¬¸ì ë¦¬ë·° ìˆ˜
      visitor_review_count = driver.find_element_by_css_selector("#app-root > div > div > div > div.place_section.GCwOh > div._3uUKd > div._37n49 > span:nth-child(2) > a > em").text
      
      #ê°€ê²Œ ë³„ì  ì ìˆ˜
      review_stars = driver.find_element_by_css_selector("#app-root > div > div > div > div.place_section.GCwOh > div._3uUKd > div._37n49 > span._1Y6hi._1A8_M > em").text

      # ë¦¬ë·° í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°.
      review_text_list = [] # ì„ì‹œ ì„ ì–¸
      
      # ë„¤ì´ë²„ ì§€ë„ ë¸”ë¡œê·¸ ë¦¬ë·° íƒ­
      # ë™ì  ì›¹ì‚¬ì´íŠ¸ì˜ ìˆœì„œê°€ ì£¼ë¬¸í•˜ê¸°, ë©”ë‰´ë³´ê¸° ë“±ì˜ ì¡´ì¬ ì—¬ë¶€ë¡œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— css selectorê°€ ì•„ë‹ˆë¼ element ì°¾ê¸°ë¡œ ì§„í–‰í•œë‹¤.
      # ì¦‰ ë¦¬ë·°íƒ­ì˜ ìš”ì†Œë¥¼ ì‘ì„±í•œë‹¤.
      review_text_crawl_list = sub_driver.find_elements_by_class_name("_3Q5_9")
      
      
      # find_elements_by_class_name ë©”ì†Œë“œë¥¼ í†µí•´  ê°€ì ¸ì˜¨ ë‚´ìš©ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ëœë‹¤.
      # ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì„ í’€ì–´ì„œ ì„ì‹œ ë°ì´í„°ì— ëª¨ì•„ ë‘ì–´ì•¼ í•œë‹¤.(forë¬¸ ì‚¬ìš©)
      for review_crawing_data in review_text_crawl_list:
        
        # ì„ì‹œë¡œ ì„ ì–¸ëœ review_text_listì— ë‹´ê¸°.
        review_text_list.append(review_crawing_data.find_element_by_tag_name("WoYOw").text)
      
      #ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ëœ í…ìŠ¤íŠ¸(í•œ ì‹ë‹¹ì— ëŒ€í•œ ì—¬ëŸ¬ ë¦¬ë·°ë“¤)ë¥¼ ë©ì–´ë¦¬ë¡œ ëª¨ì•„ì¤ë‹ˆë‹¤.
      review_text = ",".join(review_text_list)
      
      
      blog_review_list.append(review_text)
      naver_store_name_type.append(naver_store_type)  
      blog_review_count_list.append(blog_review_count)
      naver_star_review_list.append(review_stars)
      naver_visitor_review_list.append(visitor_review_count)
      
    # ë¦¬ë·°ê°€ ì—†ëŠ” ì—…ì²´ëŠ” í¬ë¡¤ë§ì— ì˜¤ë¥˜ê°€ ëœ¨ë¯€ë¡œ ì •ë¦¬í•  ê²ƒ. 
    except Exception as e1:
      print(f'{i}í–‰ì— ë¬¸ì œê°€ ë°œìƒ')
      
      #ë¦¬ë·°ê°€ ì—†ìœ¼ë¯€ë¡œ  nullì„ ì„ì‹œë¡œ ë„£ì–´ì¤€ë‹¤.
      blog_review_list.append('null')
      naver_store_name_type.append('null')
      blog_review_count_list.append('null')
      naver_star_review_list.append('null')
      naver_visitor_review_list.append('null')
      
  driver.quit()
  sub_driver.quit()
  
  df['naver_store_type'] = naver_store_name_type
  df['naver_star_point'] = naver_star_review_list
  df['naver_blog_review_count'] = blog_review_count_list
  df['naver_blog_review_text'] = blog_review_list
  df['naver_visitor_review_count'] = naver_visitor_review_list
  
  #ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ csv í˜•íƒœë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
  df.to_csv('c:/Users/j.park/Section3/real_project3/second_crawling.csv', index = False, encoding= 'cp949')
  
  print('mission completeâ—') 
  return None
              
        
        
        
        